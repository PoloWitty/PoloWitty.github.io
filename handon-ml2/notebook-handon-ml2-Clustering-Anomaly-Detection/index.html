<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="ClusteringK-means">
<meta property="og:type" content="article">
<meta property="og:title" content="Clustering &amp; Anomaly Detection">
<meta property="og:url" content="http://example.com/handon-ml2/notebook-handon-ml2-Clustering-Anomaly-Detection/index.html">
<meta property="og:site_name" content="Paolo">
<meta property="og:description" content="ClusteringK-means">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2021-04-24T15:44:17.000Z">
<meta property="article:modified_time" content="2021-04-24T15:49:14.700Z">
<meta property="article:author" content="PaoloWitty">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/handon-ml2/notebook-handon-ml2-Clustering-Anomaly-Detection/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Clustering & Anomaly Detection | Paolo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Paolo</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/handon-ml2/notebook-handon-ml2-Clustering-Anomaly-Detection/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="PaoloWitty">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Paolo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Clustering & Anomaly Detection
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-04-24 23:44:17 / 修改时间：23:49:14" itemprop="dateCreated datePublished" datetime="2021-04-24T23:44:17+08:00">2021-04-24</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/handon-ml2/" itemprop="url" rel="index"><span itemprop="name">handon-ml2</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h1><h2 id="K-means"><a href="#K-means" class="headerlink" title="K-means"></a>K-means</h2><span id="more"></span>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line">k=<span class="number">5</span></span><br><span class="line">kmeans=KMeans(n_clusters=k)</span><br><span class="line">y_pred=kmeans.fit_predict(X)</span><br></pre></td></tr></table></figure>

<p>Each instance was assigned to one of the five clusters. The <em>label</em> is the index of the cluster that this instance gets assigned to by the algorithm. ( <em>y_pred</em> is just the <em>kmeans.label_</em> , because here the <em>y_pred</em> is the training set for the kmeans)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># get the centroid of the cluster</span></span><br><span class="line">kmeans.cluster_centers_</span><br><span class="line"></span><br><span class="line"><span class="comment"># predict the new instances </span></span><br><span class="line">X_new=np.array([[<span class="number">0</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">2</span>],[-<span class="number">3</span>,<span class="number">3</span>],[-<span class="number">3</span>,<span class="number">2.5</span>]])</span><br><span class="line">kmeans.predict(X_new)</span><br><span class="line"></span><br><span class="line"><span class="comment"># get the distance from each instance to every centroid</span></span><br><span class="line">kmeans.transform(X_new)</span><br></pre></td></tr></table></figure>

<p><strong>shortage</strong> : the result can be various because of the different initial centroid. So there are some hyperparameters for this.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># if you know approximately where the centroids should be(e.g., if you ran another clustring algorithm earlier ), then you can set the *init* hyperparameter to a NumPy array containing the list of centroids, and set *n_init* to 1.</span></span><br><span class="line"></span><br><span class="line">init_centroid=np.array([[-<span class="number">3</span>,-<span class="number">3</span>],[-<span class="number">3</span>,<span class="number">2</span>],[-<span class="number">3</span>,<span class="number">1</span>],[-<span class="number">1</span>,<span class="number">2</span>],[<span class="number">0</span>,<span class="number">2</span>]])</span><br><span class="line">kmeans=KMeans(n_clusters=<span class="number">5</span>,init=init_centroid,n_init=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># and the metric to evaluate different results is *inertia*, which is the mean squared distance between each instance and its closest centroid.</span></span><br><span class="line">kmeans.inertia_</span><br><span class="line"><span class="comment">#the score method returns the negative inertia, making it easier to compare. </span></span><br></pre></td></tr></table></figure>

<h3 id="K-Means"><a href="#K-Means" class="headerlink" title="K-Means++"></a>K-Means++</h3><p>an improvement to the K-Means algorithm, instead of randomly select the initial centroid, it introduced a smarter initialization step.</p>
<ol>
<li>Take one centroid $c_1$, chosen uniformly at random from the dataset.</li>
<li>Take a new center $c_i$, choosing an instance $\mathbf{x}_i$ with probability: $D(\mathbf{x}<em>i)^2$ / $\sum\limits</em>{j=1}^{m}{D(\mathbf{x}_j)}^2$ where $D(\mathbf{x}_i)$ is the distance between the instance $\mathbf{x}_i$ and the closest centroid that was already chosen. This probability distribution ensures that instances that are further away from already chosen centroids are much more likely be selected as centroids.</li>
<li>Repeat the previous step until all $k$ centroids have been chosen.</li>
</ol>
<p>To set the initialization to K-Means++, simply set <em>init=&#x2019;k-means++&#x2019;</em>, this is actually the default initialization method.</p>
<h3 id="Accelerated-K-Means"><a href="#Accelerated-K-Means" class="headerlink" title="Accelerated K-Means"></a>Accelerated K-Means</h3><p>By exploiting the triangle inequality (AC$\leq$AB+BC), the k-means algorithm can be significantly accelerated.</p>
<p><strong>shortage</strong> : it does not support sparse data, so by default, Scikit-Learn uses &#x201C;<em>elkan</em>&#x201C;( since this variant of K-Means was invented by Charles Elkan ) for dense data, and &#x201C;<em>full</em>&#x201C;(the regular K-Means algorithm) for sparse data.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">aclr_kmeans=KMeans(algorithm=<span class="string">&apos;elkan&apos;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="Mini-Batch-K-Means"><a href="#Mini-Batch-K-Means" class="headerlink" title="Mini-Batch K-Means"></a>Mini-Batch K-Means</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> MiniBatchKMeans</span><br><span class="line">minibatch_kmeans=MiniBatchKMeans(n_clusters=<span class="number">5</span>)</span><br><span class="line">minibatch_kmeans.fit(X)</span><br></pre></td></tr></table></figure>

<p>modify for scale the data set using np.memmap</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">filename=<span class="string">&apos;my_mnist.data&apos;</span></span><br><span class="line">X_mm=np.memmap(filename,dtype=<span class="string">&apos;float32&apos;</span>,mode=<span class="string">&apos;write&apos;</span>,shape=X_train.shape)</span><br><span class="line">X_mm[:]=X_train</span><br><span class="line"></span><br><span class="line">minibatch_kmeans=MiniBatchKMeans=(n_clusters=<span class="number">10</span>,batch_size=<span class="number">19</span>)</span><br><span class="line">minibatch_kmeans.fit(X_mm)</span><br></pre></td></tr></table></figure>

<p>Although the Mini-batch K-Means algorithm is much faster than the regular K-Means algorithm, its inertia is generally slightly worse,especially as the number of clusters increases. </p>
<h3 id="Finding-the-optimal-number-of-clusters"><a href="#Finding-the-optimal-number-of-clusters" class="headerlink" title="Finding the optimal number of clusters"></a>Finding the optimal number of clusters</h3><p>We could do it by plot the curve of inertia vs the number of clusters, and find the elbow. But this is rather coarse. A more precise approach is to use the <em>silhouette score</em>, which is the mean <em>silhouette coefficient</em> over all the instances.</p>
<p>The silhouette coefficient can vary between -1 and +1. A coefficient close to +1 means that the instance is well inside its own cluster and far from other clusters, while a coefficient close to 0 means that it is close to a cluster boundary, and finally a coefficient close to -1 means that the instance may have been assigned to the wrong cluster. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> silhouette_score</span><br><span class="line">silhouette_score(X,kmeans.labels_)</span><br></pre></td></tr></table></figure>

<p> plot the silhouette score vs k:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">kmeans_per_k=[KMeans(n_clusters=k).fit(X) <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">10</span>)]</span><br><span class="line">silhouette_scores=[silhouette_score(X,model.labels_) <span class="keyword">for</span> model <span class="keyword">in</span> kmeans_per_k[<span class="number">1</span>:]]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">3</span>))</span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">2</span>, <span class="number">10</span>), silhouette_scores, <span class="string">&quot;bo-&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;$k$&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Silhouette score&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.axis([<span class="number">1.8</span>, <span class="number">8.5</span>, <span class="number">0.55</span>, <span class="number">0.7</span>])</span><br><span class="line">save_fig(<span class="string">&quot;silhouette_score_vs_k_plot&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>Another even more informative visualization is to plot the silhouette diagram. Each diagram contains one knife shape per cluster. The shape&#x2019;s height indicates the number of instances the cluster contains, and its width represents the sorted silhouette coefficients of the instances in the cluster (wider is better ). The dashed line indicates the mean silhouette coefficient.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> silhouette_samples</span><br><span class="line"><span class="keyword">from</span> matplotlib.ticker <span class="keyword">import</span> FixedLocator, FixedFormatter</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">11</span>, <span class="number">9</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> (<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>):</span><br><span class="line">    plt.subplot(<span class="number">2</span>, <span class="number">2</span>, k - <span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    y_pred = kmeans_per_k[k - <span class="number">1</span>].labels_</span><br><span class="line">    silhouette_coefficients = silhouette_samples(X, y_pred)</span><br><span class="line"></span><br><span class="line">    padding = <span class="built_in">len</span>(X) // <span class="number">30</span></span><br><span class="line">    pos = padding</span><br><span class="line">    ticks = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">        coeffs = silhouette_coefficients[y_pred == i]</span><br><span class="line">        coeffs.sort()</span><br><span class="line"></span><br><span class="line">        color = mpl.cm.Spectral(i / k)</span><br><span class="line">        plt.fill_betweenx(np.arange(pos, pos + <span class="built_in">len</span>(coeffs)), <span class="number">0</span>, coeffs,</span><br><span class="line">                          facecolor=color, edgecolor=color, alpha=<span class="number">0.7</span>)</span><br><span class="line">        ticks.append(pos + <span class="built_in">len</span>(coeffs) // <span class="number">2</span>)</span><br><span class="line">        pos += <span class="built_in">len</span>(coeffs) + padding</span><br><span class="line"></span><br><span class="line">    plt.gca().yaxis.set_major_locator(FixedLocator(ticks))</span><br><span class="line">    plt.gca().yaxis.set_major_formatter(FixedFormatter(<span class="built_in">range</span>(k)))</span><br><span class="line">    <span class="keyword">if</span> k <span class="keyword">in</span> (<span class="number">3</span>, <span class="number">5</span>):</span><br><span class="line">        plt.ylabel(<span class="string">&quot;Cluster&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> k <span class="keyword">in</span> (<span class="number">5</span>, <span class="number">6</span>):</span><br><span class="line">        plt.gca().set_xticks([-<span class="number">0.1</span>, <span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.6</span>, <span class="number">0.8</span>, <span class="number">1</span>])</span><br><span class="line">        plt.xlabel(<span class="string">&quot;Silhouette Coefficient&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        plt.tick_params(labelbottom=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    plt.axvline(x=silhouette_scores[k - <span class="number">2</span>], color=<span class="string">&quot;red&quot;</span>, linestyle=<span class="string">&quot;--&quot;</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;$k={}$&quot;</span>.<span class="built_in">format</span>(k), fontsize=<span class="number">16</span>)</span><br><span class="line"></span><br><span class="line">save_fig(<span class="string">&quot;silhouette_analysis_plot&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<h3 id="Limits-of-K-Means"><a href="#Limits-of-K-Means" class="headerlink" title="Limits of K-Means"></a>Limits of K-Means</h3><ul>
<li>need to run the algorithm several times to avoid suboptimal solutions</li>
<li>need to specify the number of clusters</li>
<li>does not behave very well when the clusters have varying sizes, different densities, or nonspherical shapes.</li>
</ul>
<h3 id="using-K-Means-for-Semi-Supervised-Learning"><a href="#using-K-Means-for-Semi-Supervised-Learning" class="headerlink" title="using K-Means for Semi-Supervised Learning"></a>using K-Means for Semi-Supervised Learning</h3><ul>
<li><strong>representative images</strong> : find the representative instances in each cluster and label them. Use these labeled instances for supervised learning algorithm instead of randomly choosing some instances for labeling</li>
<li><strong>label propagation</strong> : label the representative instances in each cluster and label all the instance( or 20%, or 50% close to the labeled instance ) in the same cluster the same label as the representative instance </li>
</ul>
<h2 id="DBSCAN"><a href="#DBSCAN" class="headerlink" title="DBSCAN"></a>DBSCAN</h2><ul>
<li>If an instance has at least <em>min_samples</em> instances in its $\epsilon$-neighborhood (including itself ), then it is considered a <em>core instance</em>. </li>
<li>Any instance that is not a core instance and does not have one in its neighborhood is considered an anomaly, which will be -1 in <em>dbscan.labels_</em></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> DBSCAN</span><br><span class="line">dbscan=DBSCAN(eps=<span class="number">0.05</span>,min_samples=<span class="number">5</span>)</span><br><span class="line">dbscan.fit(X)</span><br></pre></td></tr></table></figure>

<p>Note: the <em>DBSCAN</em> class does not have a <em>predict()</em> method. But it&#x2019;s  not hard to implement. Take the <em>KNeighborsClassifier</em> as an example</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line">knn=KNeighborsClassifier(n_neighbors=<span class="number">50</span>)</span><br><span class="line">knn.fit(dbscan.components_,dbscan.labels_[dbscan.core_sample_indices_])</span><br><span class="line"><span class="comment"># The labels of all the instances are available in the *labels_* (-1 for anomaly). The indices of the core instances are available in the *core_sample_indices_* and the core instances themselves are available in the *components_*.</span></span><br><span class="line">X_new=np.array([[-<span class="number">0.5</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0.5</span>],[<span class="number">1</span>,-<span class="number">0.1</span>],[<span class="number">2</span>,<span class="number">1</span>]])</span><br><span class="line">knn.predict(X_new)</span><br><span class="line">knn.predict_proba(X_new)</span><br></pre></td></tr></table></figure>

<p><strong>Shortage</strong> : </p>
<ul>
<li>If the density varies significantly across the clusters, however, it can be impossible for it to capture all the clusters properly.</li>
<li>Even though its complexity of time is roughly linear, but Scikit-Learn&#x2019;s implementation can require up to O($m^2$) memory if $eps$ is large</li>
</ul>
<h1 id="Gaussian-Mixtures"><a href="#Gaussian-Mixtures" class="headerlink" title="Gaussian Mixtures"></a>Gaussian Mixtures</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.mixture <span class="keyword">import</span> GaussianMixture</span><br><span class="line"></span><br><span class="line">gm=GaussianMixture(n_components=<span class="number">3</span>,n_init=<span class="number">10</span>)<span class="comment"># n_init =10 because Gaussian Mixture can end up converging to poor solutions just like K-Means</span></span><br><span class="line">gm.fit(X)</span><br></pre></td></tr></table></figure>

<p>Gaussian Mixture model is a <em>generative model</em>, meaning that you can sample new instance from it.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_new,y_new=gam.sample(<span class="number">10</span>)</span><br><span class="line">gm.score_samples(X)</span><br></pre></td></tr></table></figure>

<p>In order to estimate the density of the model at any given location, you can use <em>score_samples()</em> method, which return the <strong>log</strong> of the <em>probability density function( PDF )</em> at the given location. </p>
<h2 id="Anomaly-Detection-Using-Gaussian-Mixture"><a href="#Anomaly-Detection-Using-Gaussian-Mixture" class="headerlink" title="Anomaly Detection Using Gaussian Mixture"></a>Anomaly Detection Using Gaussian Mixture</h2><p>Base idea: any instance located in a low-density region can be considered an anomaly. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">densities = gm.score_samples(X)</span><br><span class="line">density_threshold=np.percentile(densities,<span class="number">4</span>)<span class="comment"># &#x540E;4%&#x5206;&#x4F4D;&#x6570;</span></span><br><span class="line">anomalies=X[densities&lt;density_threshold]</span><br></pre></td></tr></table></figure>

<h2 id="Selecting-the-Number-of-Clusters"><a href="#Selecting-the-Number-of-Clusters" class="headerlink" title="Selecting the Number of Clusters"></a>Selecting the Number of Clusters</h2><p>The inertia and the silhouette score can no longer be used in Gaussian mixtures because they are not reliable when the clusters are not spherical or have different sizes. </p>
<p>The new criterions are <em>Bayesian information criterion(BIC)</em> and <em>Akaike information criterion(AIC)</em>.<br>$$<br>BIC=log(m)p-2log(\hat L)\<br>AIC=2p-2log(\hat L)<br>$$<br>$m$ is the number of instances, $p$ is the number of parameters learned by the model, $\hat L$ is the maximum value of the likelihood function of the model.</p>
<p>BIC and AIC are similar most of the time, but when they differ, the model selected by BIC tends to be simpler, but tend not fit the data quite as well.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gm.bic(X)</span><br><span class="line">gm.aic(X)</span><br></pre></td></tr></table></figure>

<p>compute them manually:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">n_clusters = <span class="number">3</span></span><br><span class="line">n_dims = <span class="number">2</span></span><br><span class="line">n_params_for_weights = n_clusters - <span class="number">1</span></span><br><span class="line">n_params_for_means = n_clusters * n_dims</span><br><span class="line">n_params_for_covariance = n_clusters * n_dims * (n_dims + <span class="number">1</span>) // <span class="number">2</span></span><br><span class="line">n_params = n_params_for_weights + n_params_for_means + n_params_for_covariance</span><br><span class="line">max_log_likelihood = gm.score(X) * <span class="built_in">len</span>(X) <span class="comment"># log(L^)</span></span><br><span class="line">bic = np.log(<span class="built_in">len</span>(X)) * n_params - <span class="number">2</span> * max_log_likelihood</span><br><span class="line">aic = <span class="number">2</span> * n_params - <span class="number">2</span> * max_log_likelihood</span><br></pre></td></tr></table></figure>

<p>There&#x2019;s one weight per cluster, but the sum must be equal to 1, so we have one degree of freedom less, hence the -1. Similarly, the degrees of freedom for an $n \times n$ covariance matrix is not $n^2$, but $1 + 2 + \dots + n = \dfrac{n (n+1)}{2}$.</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/handon-ml2/notebook-handon-ml2-Decision-Trees/" rel="prev" title="Decision Trees">
      <i class="fa fa-chevron-left"></i> Decision Trees
    </a></div>
      <div class="post-nav-item">
    <a href="/handon-ml2/notebook-handon-ml2-Classification/" rel="next" title="Classification">
      Classification <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Clustering"><span class="nav-number">1.</span> <span class="nav-text">Clustering</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#K-means"><span class="nav-number">1.1.</span> <span class="nav-text">K-means</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#K-Means"><span class="nav-number">1.1.1.</span> <span class="nav-text">K-Means++</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Accelerated-K-Means"><span class="nav-number">1.1.2.</span> <span class="nav-text">Accelerated K-Means</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Mini-Batch-K-Means"><span class="nav-number">1.1.3.</span> <span class="nav-text">Mini-Batch K-Means</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Finding-the-optimal-number-of-clusters"><span class="nav-number">1.1.4.</span> <span class="nav-text">Finding the optimal number of clusters</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Limits-of-K-Means"><span class="nav-number">1.1.5.</span> <span class="nav-text">Limits of K-Means</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#using-K-Means-for-Semi-Supervised-Learning"><span class="nav-number">1.1.6.</span> <span class="nav-text">using K-Means for Semi-Supervised Learning</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DBSCAN"><span class="nav-number">1.2.</span> <span class="nav-text">DBSCAN</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Gaussian-Mixtures"><span class="nav-number">2.</span> <span class="nav-text">Gaussian Mixtures</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Anomaly-Detection-Using-Gaussian-Mixture"><span class="nav-number">2.1.</span> <span class="nav-text">Anomaly Detection Using Gaussian Mixture</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Selecting-the-Number-of-Clusters"><span class="nav-number">2.2.</span> <span class="nav-text">Selecting the Number of Clusters</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">PaoloWitty</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">12</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">PaoloWitty</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

</body>
</html>
